{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04aa90b4",
   "metadata": {},
   "source": [
    "# Parallel efeature extraction using multiprocessing/scoop\n",
    "\n",
    "You can use multiprocessing or scoop (Scalable COncurrent Operations in Python) packages to extract features in parallel. This is a simple example of how to do it using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2500a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efel\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main\"\"\"\n",
    "\n",
    "    traces = []\n",
    "    for filename in ['example_trace1.txt', 'example_trace2.txt']:\n",
    "        # Use numpy to read the trace data from the txt file\n",
    "        data = numpy.loadtxt(filename)\n",
    "\n",
    "        # Time is the first column\n",
    "        time = data[:, 0]\n",
    "        # Voltage is the second column\n",
    "        voltage = data[:, 1]\n",
    "\n",
    "        # Now we will construct the datastructure that will be passed to eFEL\n",
    "\n",
    "        # A 'trace' is a dictionary\n",
    "        trace1 = {}\n",
    "\n",
    "        # Set the 'T' (=time) key of the trace\n",
    "        trace1['T'] = time\n",
    "\n",
    "        # Set the 'V' (=voltage) key of the trace\n",
    "        trace1['V'] = voltage\n",
    "\n",
    "        # Set the 'stim_start' (time at which a stimulus starts, in ms)\n",
    "        # key of the trace\n",
    "        # Warning: this need to be a list (with one element)\n",
    "        trace1['stim_start'] = [700]\n",
    "\n",
    "        # Set the 'stim_end' (time at which a stimulus end) key of the trace\n",
    "        # Warning: this need to be a list (with one element)\n",
    "        trace1['stim_end'] = [2700]\n",
    "\n",
    "        # Multiple traces can be passed to the eFEL at the same time, so the\n",
    "        # argument should be a list\n",
    "        traces.append(trace1)\n",
    "\n",
    "    # Now we pass 'traces' to the efel and ask it to calculate the feature\n",
    "    # values\n",
    "\n",
    "    ## Using mutliprocessing\n",
    "    import multiprocessing\n",
    "    pool = multiprocessing.Pool()\n",
    "    traces_results = efel.get_feature_values(\n",
    "        traces, [\n",
    "            'AP_amplitude', 'voltage_base'], parallel_map=pool.map)\n",
    "\n",
    "    # The return value is a list of trace_results, every trace_results\n",
    "    # corresponds to one trace in the 'traces' list above (in same order)\n",
    "    for trace_number, trace_results in enumerate(traces_results):\n",
    "        print(\"Results for trace %d\" % (trace_number + 1))\n",
    "        # trace_result is a dictionary, with as keys the requested features\n",
    "        for feature_name, feature_values in trace_results.items():\n",
    "            print(\"Feature %s has the following values: %s\" % \\\n",
    "                (feature_name, ', '.join([str(x) for x in feature_values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f6611",
   "metadata": {},
   "source": [
    "If you want to use scoop, it is advisable to use a python script e.g. the `scoop_example.py` (present in the `parallel` example folder) instead of the notebook. You can launch the script e.g. using \n",
    "\n",
    "`python -m scoop -n 4 scoop_example.py`\n",
    "\n",
    "where arguments are:\n",
    "\n",
    "`-m scoop`: The `-m` flag tells Python to run a module as a script. In this case, the module is scoop, which is a library for distributed computing. Running SCOOP as a module initializes its environment, preparing it for distributed execution.\n",
    "\n",
    "`-n 4`: This option specifies the number of worker processes that scoop should use for executing tasks. In this case, 4 workers are requested. scoop will distribute tasks among these workers, allowing for parallel execution."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "insiders2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
